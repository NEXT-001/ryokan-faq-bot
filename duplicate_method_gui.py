#!/usr/bin/env python3
"""
é‡è¤‡ãƒ¡ã‚½ãƒƒãƒ‰æ¤œå‡ºGUIï¼ˆStreamlitç‰ˆï¼‰
ãƒ–ãƒ©ã‚¦ã‚¶ã§ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ã«é‡è¤‡ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ç¢ºèªã§ãã¾ã™
"""

import streamlit as st
import os
import ast
import hashlib
import difflib
from collections import defaultdict
from typing import Dict, List, Tuple
import pandas as pd

# ãƒ¡ã‚¤ãƒ³ã®æ¤œå‡ºãƒ­ã‚¸ãƒƒã‚¯ã¯ä¸Šè¨˜ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‹ã‚‰å†åˆ©ç”¨
class MethodInfo:
    """ãƒ¡ã‚½ãƒƒãƒ‰æƒ…å ±ã‚’æ ¼ç´ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    def __init__(self, name: str, file_path: str, line_number: int, 
                 signature: str, body: str, docstring: str = ""):
        self.name = name
        self.file_path = file_path
        self.line_number = line_number
        self.signature = signature
        self.body = body
        self.docstring = docstring
        self.body_hash = hashlib.md5(self.normalize_body().encode()).hexdigest()
        
    def normalize_body(self) -> str:
        """ãƒ¡ã‚½ãƒƒãƒ‰æœ¬ä½“ã‚’æ­£è¦åŒ–ï¼ˆã‚³ãƒ¡ãƒ³ãƒˆã€ç©ºç™½ã‚’é™¤å»ï¼‰"""
        lines = []
        for line in self.body.split('\n'):
            if '#' in line:
                line = line[:line.index('#')]
            line = line.strip()
            if line:
                lines.append(line)
        return '\n'.join(lines)
    
    def similarity_score(self, other: 'MethodInfo') -> float:
        """ä»–ã®ãƒ¡ã‚½ãƒƒãƒ‰ã¨ã®é¡ä¼¼åº¦ã‚’è¨ˆç®—ï¼ˆ0.0-1.0ï¼‰"""
        if self.body_hash == other.body_hash:
            return 1.0
        
        normalized_self = self.normalize_body()
        normalized_other = other.normalize_body()
        
        if not normalized_self or not normalized_other:
            return 0.0
        
        similarity = difflib.SequenceMatcher(None, normalized_self, normalized_other).ratio()
        return similarity

class MethodAnalyzer(ast.NodeVisitor):
    """ASTã‚’ä½¿ã£ã¦Pythonãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ¡ã‚½ãƒƒãƒ‰ã‚’æŠ½å‡º"""
    
    def __init__(self, file_path: str, source_code: str):
        self.file_path = file_path
        self.source_lines = source_code.split('\n')
        self.methods = []
        
    def visit_FunctionDef(self, node: ast.FunctionDef):
        """é–¢æ•°å®šç¾©ã‚’è¨ªå•"""
        self._extract_method_info(node)
        self.generic_visit(node)
    
    def visit_AsyncFunctionDef(self, node: ast.AsyncFunctionDef):
        """éåŒæœŸé–¢æ•°å®šç¾©ã‚’è¨ªå•"""
        self._extract_method_info(node)
        self.generic_visit(node)
    
    def _extract_method_info(self, node):
        """ãƒ¡ã‚½ãƒƒãƒ‰æƒ…å ±ã‚’æŠ½å‡º"""
        name = node.name
        line_number = node.lineno
        signature = self._build_signature(node)
        
        start_line = node.lineno - 1
        end_line = node.end_lineno - 1 if node.end_lineno else start_line
        body_lines = self.source_lines[start_line:end_line + 1]
        body = '\n'.join(body_lines)
        
        docstring = ast.get_docstring(node) or ""
        
        method_info = MethodInfo(
            name=name,
            file_path=self.file_path,
            line_number=line_number,
            signature=signature,
            body=body,
            docstring=docstring
        )
        
        self.methods.append(method_info)
    
    def _build_signature(self, node) -> str:
        """ãƒ¡ã‚½ãƒƒãƒ‰ã‚·ã‚°ãƒãƒãƒ£ã‚’æ§‹ç¯‰"""
        args = []
        
        for arg in node.args.args:
            args.append(arg.arg)
        
        default_offset = len(node.args.args) - len(node.args.defaults)
        for i, default in enumerate(node.args.defaults):
            arg_index = default_offset + i
            if arg_index < len(node.args.args):
                args[arg_index] += "=default"
        
        if node.args.vararg:
            args.append(f"*{node.args.vararg.arg}")
        
        if node.args.kwarg:
            args.append(f"**{node.args.kwarg.arg}")
        
        return f"{node.name}({', '.join(args)})"

class StreamlitDuplicateFinder:
    """Streamlitç”¨ã®é‡è¤‡ãƒ¡ã‚½ãƒƒãƒ‰æ¤œå‡ºã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.all_methods: List[MethodInfo] = []
        self.duplicates = {}
        
    def scan_directory(self, directory_path: str, exclude_patterns: List[str]):
        """ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ã‚¹ã‚­ãƒ£ãƒ³ã—ã¦ãƒ¡ã‚½ãƒƒãƒ‰ã‚’åé›†"""
        self.all_methods = []
        
        for root, dirs, files in os.walk(directory_path):
            dirs[:] = [d for d in dirs if not any(pattern in d for pattern in exclude_patterns)]
            
            for file in files:
                if file.endswith('.py'):
                    file_path = os.path.join(root, file)
                    self._analyze_file(file_path)
        
        return len(self.all_methods)
    
    def _analyze_file(self, file_path: str):
        """å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†æ"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                source_code = f.read()
            
            tree = ast.parse(source_code)
            analyzer = MethodAnalyzer(file_path, source_code)
            analyzer.visit(tree)
            
            self.all_methods.extend(analyzer.methods)
            
        except Exception as e:
            st.warning(f"ãƒ•ã‚¡ã‚¤ãƒ«åˆ†æã‚¨ãƒ©ãƒ¼ {file_path}: {e}")
    
    def find_duplicates(self, similarity_threshold: float = 0.8):
        """é‡è¤‡ãƒ»é¡ä¼¼ãƒ¡ã‚½ãƒƒãƒ‰ã‚’æ¤œå‡º"""
        methods_by_name = defaultdict(list)
        for method in self.all_methods:
            methods_by_name[method.name].append(method)
        
        self.duplicates = {}
        
        for method_name, methods in methods_by_name.items():
            if len(methods) > 1:
                similar_groups = self._group_similar_methods(methods, similarity_threshold)
                if similar_groups:
                    self.duplicates[method_name] = similar_groups
        
        return self.duplicates
    
    def _group_similar_methods(self, methods: List[MethodInfo], threshold: float):
        """ãƒ¡ã‚½ãƒƒãƒ‰ã‚’é¡ä¼¼åº¦ã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–"""
        groups = []
        processed = set()
        
        for i, method1 in enumerate(methods):
            if i in processed:
                continue
                
            current_group = [method1]
            processed.add(i)
            
            for j, method2 in enumerate(methods):
                if j <= i or j in processed:
                    continue
                
                similarity = method1.similarity_score(method2)
                if similarity >= threshold:
                    current_group.append(method2)
                    processed.add(j)
            
            if len(current_group) > 1:
                groups.append(current_group)
        
        return groups

def main():
    """Streamlitã‚¢ãƒ—ãƒªã®ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    st.set_page_config(
        page_title="é‡è¤‡ãƒ¡ã‚½ãƒƒãƒ‰æ¤œå‡ºãƒ„ãƒ¼ãƒ«",
        page_icon="ğŸ”",
        layout="wide"
    )
    
    st.title("ğŸ” é‡è¤‡ãƒ¡ã‚½ãƒƒãƒ‰æ¤œå‡ºãƒ„ãƒ¼ãƒ«")
    st.markdown("ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå†…ã®é‡è¤‡ãƒ»é¡ä¼¼ãƒ¡ã‚½ãƒƒãƒ‰ã‚’æ¤œå‡ºã—ã¦çµ±åˆã®æ©Ÿä¼šã‚’è¦‹ã¤ã‘ã¾ã™")
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§ã®è¨­å®š
    with st.sidebar:
        st.header("âš™ï¸ è¨­å®š")
        
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®é¸æŠ
        project_path = st.text_input(
            "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‘ã‚¹", 
            value=".",
            help="åˆ†æã™ã‚‹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹"
        )
        
        # é¡ä¼¼åº¦é–¾å€¤
        similarity_threshold = st.slider(
            "é¡ä¼¼åº¦é–¾å€¤",
            min_value=0.5,
            max_value=1.0,
            value=0.8,
            step=0.05,
            help="ã“ã®å€¤ä»¥ä¸Šã®é¡ä¼¼åº¦ã‚’æŒã¤ãƒ¡ã‚½ãƒƒãƒ‰ã‚’é‡è¤‡ã¨ã—ã¦æ¤œå‡º"
        )
        
        # é™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³
        default_excludes = [
            '__pycache__', '.git', '.venv', 'venv', 'env',
            'node_modules', '.pytest_cache', 'build', 'dist'
        ]
        
        exclude_patterns = st.multiselect(
            "é™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³",
            options=default_excludes + ['test', 'tests', 'migrations'],
            default=default_excludes,
            help="ã‚¹ã‚­ãƒ£ãƒ³ã‹ã‚‰é™¤å¤–ã™ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¿ãƒ¼ãƒ³"
        )
        
        # ã‚¹ã‚­ãƒ£ãƒ³å®Ÿè¡Œãƒœã‚¿ãƒ³
        scan_button = st.button("ğŸ” ã‚¹ã‚­ãƒ£ãƒ³å®Ÿè¡Œ", type="primary")
    
    # ãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢
    if scan_button:
        if not os.path.exists(project_path):
            st.error(f"æŒ‡å®šã•ã‚ŒãŸãƒ‘ã‚¹ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {project_path}")
            return
        
        # ã‚¹ã‚­ãƒ£ãƒ³å®Ÿè¡Œ
        finder = StreamlitDuplicateFinder()
        
        with st.spinner("ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’ã‚¹ã‚­ãƒ£ãƒ³ä¸­..."):
            method_count = finder.scan_directory(project_path, exclude_patterns)
        
        st.success(f"âœ… ã‚¹ã‚­ãƒ£ãƒ³å®Œäº†ï¼ {method_count}å€‹ã®ãƒ¡ã‚½ãƒƒãƒ‰ã‚’æ¤œå‡ºã—ã¾ã—ãŸ")
        
        # é‡è¤‡æ¤œå‡º
        with st.spinner("é‡è¤‡ãƒ¡ã‚½ãƒƒãƒ‰ã‚’æ¤œç´¢ä¸­..."):
            duplicates = finder.find_duplicates(similarity_threshold)
        
        # çµæœè¡¨ç¤º
        if not duplicates:
            st.info("ğŸ‰ é‡è¤‡ãƒ¡ã‚½ãƒƒãƒ‰ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼")
            
            # çµ±è¨ˆæƒ…å ±
            with st.expander("ğŸ“Š çµ±è¨ˆæƒ…å ±"):
                st.metric("ç·ãƒ¡ã‚½ãƒƒãƒ‰æ•°", method_count)
                st.metric("é‡è¤‡ãƒ¡ã‚½ãƒƒãƒ‰", 0)
                st.metric("é‡è¤‡ç‡", "0%")
        else:
            # é‡è¤‡ãƒ¡ã‚½ãƒƒãƒ‰ã®è©³ç´°è¡¨ç¤º
            total_duplicates = sum(len(groups) for groups in duplicates.values())
            duplicate_methods = sum(sum(len(group) for group in groups) for groups in duplicates.values())
            
            # ã‚µãƒãƒªãƒ¼
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("ç·ãƒ¡ã‚½ãƒƒãƒ‰æ•°", method_count)
            with col2:
                st.metric("é‡è¤‡ã‚°ãƒ«ãƒ¼ãƒ—æ•°", total_duplicates)
            with col3:
                st.metric("é‡è¤‡ãƒ¡ã‚½ãƒƒãƒ‰æ•°", duplicate_methods)
            with col4:
                duplicate_rate = (duplicate_methods / method_count * 100) if method_count > 0 else 0
                st.metric("é‡è¤‡ç‡", f"{duplicate_rate:.1f}%")
            
            st.markdown("---")
            
            # é‡è¤‡ãƒ¡ã‚½ãƒƒãƒ‰ã®è©³ç´°
            for method_name, groups in duplicates.items():
                st.subheader(f"ğŸ”„ ãƒ¡ã‚½ãƒƒãƒ‰å: `{method_name}`")
                
                for i, group in enumerate(groups, 1):
                    with st.expander(f"ã‚°ãƒ«ãƒ¼ãƒ— {i} ({len(group)}å€‹ã®ãƒ¡ã‚½ãƒƒãƒ‰)", expanded=True):
                        
                        # ã‚°ãƒ«ãƒ¼ãƒ—å†…ã®ãƒ¡ã‚½ãƒƒãƒ‰ä¸€è¦§
                        method_data = []
                        for j, method in enumerate(group):
                            relative_path = os.path.relpath(method.file_path, project_path)
                            method_data.append({
                                "No.": j + 1,
                                "ã‚·ã‚°ãƒãƒãƒ£": method.signature,
                                "ãƒ•ã‚¡ã‚¤ãƒ«": relative_path,
                                "è¡Œç•ªå·": method.line_number,
                                "Docstring": method.docstring[:50] + "..." if len(method.docstring) > 50 else method.docstring
                            })
                        
                        # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã§è¡¨ç¤º
                        df = pd.DataFrame(method_data)
                        st.dataframe(df, use_container_width=True)
                        
                        # é¡ä¼¼åº¦ãƒãƒˆãƒªãƒƒã‚¯ã‚¹
                        if len(group) > 1:
                            st.write("**é¡ä¼¼åº¦ãƒãƒˆãƒªãƒƒã‚¯ã‚¹:**")
                            similarity_data = []
                            for k, method1 in enumerate(group):
                                row = {}
                                for l, method2 in enumerate(group):
                                    if k == l:
                                        row[f"ãƒ¡ã‚½ãƒƒãƒ‰{l+1}"] = "100%"
                                    elif k < l:
                                        similarity = method1.similarity_score(method2)
                                        row[f"ãƒ¡ã‚½ãƒƒãƒ‰{l+1}"] = f"{similarity:.1%}"
                                    else:
                                        row[f"ãƒ¡ã‚½ãƒƒãƒ‰{l+1}"] = "-"
                                similarity_data.append(row)
                            
                            similarity_df = pd.DataFrame(similarity_data, 
                                                       index=[f"ãƒ¡ã‚½ãƒƒãƒ‰{i+1}" for i in range(len(group))])
                            st.dataframe(similarity_df, use_container_width=True)
                        
                        # æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
                        st.write("**ğŸ¯ æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³:**")
                        
                        # å®Œå…¨ä¸€è‡´ã®å ´åˆ
                        if len(group) > 1:
                            max_similarity = max(
                                group[i].similarity_score(group[j]) 
                                for i in range(len(group)) 
                                for j in range(i+1, len(group))
                            )
                            
                            if max_similarity >= 0.98:
                                st.error("ğŸš¨ ã»ã¼å®Œå…¨ä¸€è‡´: 1ã¤ã‚’æ®‹ã—ã¦ä»–ã‚’å‰Šé™¤ã™ã‚‹ã“ã¨ã‚’æ¤œè¨")
                            elif max_similarity >= 0.9:
                                st.warning("âš ï¸ éå¸¸ã«é«˜ã„é¡ä¼¼åº¦: çµ±åˆã‚’å¼·ãæ¨å¥¨")
                            elif max_similarity >= similarity_threshold:
                                st.info("ğŸ’¡ é¡ä¼¼åº¦ãŒé«˜ã„: å…±é€šåŒ–ã‚’æ¤œè¨")
                        
                        # å„ãƒ¡ã‚½ãƒƒãƒ‰ã®ã‚³ãƒ¼ãƒ‰è¡¨ç¤ºï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
                        show_code = st.checkbox(f"ã‚³ãƒ¼ãƒ‰ã‚’è¡¨ç¤º (ã‚°ãƒ«ãƒ¼ãƒ— {i})", key=f"show_code_{method_name}_{i}")
                        if show_code:
                            for j, method in enumerate(group):
                                st.write(f"**ãƒ¡ã‚½ãƒƒãƒ‰ {j+1}: {method.signature}**")
                                st.code(method.body, language="python")
                                st.markdown("---")
            
            # å…¨ä½“çš„ãªæ¨å¥¨äº‹é …
            st.markdown("---")
            st.subheader("ğŸ“‹ å…¨ä½“çš„ãªæ¨å¥¨äº‹é …")
            
            recommendations = []
            
            # å®Œå…¨ä¸€è‡´ã®ãƒ¡ã‚½ãƒƒãƒ‰æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
            exact_matches = 0
            high_similarity = 0
            
            for groups in duplicates.values():
                for group in groups:
                    if len(group) > 1:
                        max_sim = max(
                            group[i].similarity_score(group[j]) 
                            for i in range(len(group)) 
                            for j in range(i+1, len(group))
                        )
                        if max_sim >= 0.98:
                            exact_matches += 1
                        elif max_sim >= 0.9:
                            high_similarity += 1
            
            if exact_matches > 0:
                recommendations.append(f"ğŸš¨ **ç·Šæ€¥**: {exact_matches}å€‹ã®ã‚°ãƒ«ãƒ¼ãƒ—ã§å®Œå…¨ä¸€è‡´ã‚’æ¤œå‡ºã€‚å³åº§ã«çµ±åˆã‚’æ¤œè¨")
            
            if high_similarity > 0:
                recommendations.append(f"âš ï¸ **é‡è¦**: {high_similarity}å€‹ã®ã‚°ãƒ«ãƒ¼ãƒ—ã§é«˜ã„é¡ä¼¼åº¦ã‚’æ¤œå‡ºã€‚çµ±åˆã‚’æ¤œè¨")
            
            if duplicate_rate > 20:
                recommendations.append("ğŸ“Š **å…¨ä½“**: é‡è¤‡ç‡ãŒ20%ã‚’è¶…ãˆã¦ã„ã¾ã™ã€‚ã‚³ãƒ¼ãƒ‰ã®æ•´ç†ã‚’æ¨å¥¨")
            
            recommendations.extend([
                "ğŸ” **ç¢ºèª**: å„ãƒ¡ã‚½ãƒƒãƒ‰ãŒæœ¬å½“ã«å¿…è¦ã‹æ¤œè¨",
                "ğŸ—ï¸ **ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°**: å…±é€šéƒ¨åˆ†ã‚’æŠ½å‡ºã—ã¦ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°åŒ–",
                "ğŸ“ **å‘½å**: é¡ä¼¼æ©Ÿèƒ½ã®ãƒ¡ã‚½ãƒƒãƒ‰ã¯çµ±ä¸€çš„ãªå‘½åè¦å‰‡ã‚’æ¡ç”¨",
                "ğŸ§ª **ãƒ†ã‚¹ãƒˆ**: çµ±åˆå‰ã«æ—¢å­˜ã®ãƒ†ã‚¹ãƒˆãŒé€šã‚‹ã“ã¨ã‚’ç¢ºèª"
            ])
            
            for rec in recommendations:
                st.markdown(f"- {rec}")
            
            # ãƒ¬ãƒãƒ¼ãƒˆã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            st.markdown("---")
            st.subheader("ğŸ“„ ãƒ¬ãƒãƒ¼ãƒˆã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ")
            
            # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
            report_text = generate_text_report(duplicates, project_path, method_count, duplicate_rate)
            
            st.download_button(
                label="ğŸ“‹ ãƒ†ã‚­ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                data=report_text,
                file_name=f"duplicate_methods_report_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.txt",
                mime="text/plain"
            )
            
            # CSVå½¢å¼ã§ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
            csv_data = generate_csv_report(duplicates, project_path)
            st.download_button(
                label="ğŸ“Š CSVãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                data=csv_data,
                file_name=f"duplicate_methods_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.csv",
                mime="text/csv"
            )
    
    else:
        # åˆæœŸç”»é¢
        st.info("""
        ### ğŸš€ ä½¿ã„æ–¹
        
        1. **ã‚µã‚¤ãƒ‰ãƒãƒ¼**ã§ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‘ã‚¹ã¨è¨­å®šã‚’å…¥åŠ›
        2. **é¡ä¼¼åº¦é–¾å€¤**ã‚’èª¿æ•´ï¼ˆæ¨å¥¨: 0.8ï¼‰
        3. **ã‚¹ã‚­ãƒ£ãƒ³å®Ÿè¡Œ**ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯
        4. çµæœã‚’ç¢ºèªã—ã¦é‡è¤‡ãƒ¡ã‚½ãƒƒãƒ‰ã‚’çµ±åˆ
        
        ### ğŸ’¡ ãƒ’ãƒ³ãƒˆ
        
        - **é¡ä¼¼åº¦é–¾å€¤ 0.9ä»¥ä¸Š**: ã»ã¼åŒä¸€ã®ãƒ¡ã‚½ãƒƒãƒ‰
        - **é¡ä¼¼åº¦é–¾å€¤ 0.8-0.9**: çµ±åˆã‚’æ¤œè¨ã™ã¹ããƒ¡ã‚½ãƒƒãƒ‰  
        - **é¡ä¼¼åº¦é–¾å€¤ 0.7-0.8**: å…±é€šåŒ–ã®å¯èƒ½æ€§ãŒã‚ã‚‹ãƒ¡ã‚½ãƒƒãƒ‰
        """)

def generate_text_report(duplicates: Dict, project_path: str, total_methods: int, duplicate_rate: float) -> str:
    """ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ã®ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
    lines = []
    lines.append("=" * 80)
    lines.append("é‡è¤‡ãƒ¡ã‚½ãƒƒãƒ‰æ¤œå‡ºãƒ¬ãƒãƒ¼ãƒˆ")
    lines.append("=" * 80)
    lines.append(f"ç”Ÿæˆæ—¥æ™‚: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}")
    lines.append(f"ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ: {project_path}")
    lines.append(f"ç·ãƒ¡ã‚½ãƒƒãƒ‰æ•°: {total_methods}")
    lines.append(f"é‡è¤‡ç‡: {duplicate_rate:.1f}%")
    lines.append("")
    
    if not duplicates:
        lines.append("âœ… é‡è¤‡ãƒ¡ã‚½ãƒƒãƒ‰ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return '\n'.join(lines)
    
    for method_name, groups in duplicates.items():
        lines.append(f"## ãƒ¡ã‚½ãƒƒãƒ‰å: {method_name}")
        lines.append("-" * 40)
        
        for i, group in enumerate(groups, 1):
            lines.append(f"\n### ã‚°ãƒ«ãƒ¼ãƒ— {i} ({len(group)}å€‹)")
            
            for j, method in enumerate(group, 1):
                relative_path = os.path.relpath(method.file_path, project_path)
                lines.append(f"  {j}. {method.signature}")
                lines.append(f"     ğŸ“ {relative_path}:{method.line_number}")
                
                if method.docstring:
                    first_line = method.docstring.split('\n')[0].strip()
                    lines.append(f"     ğŸ“ {first_line}")
            
            # é¡ä¼¼åº¦æƒ…å ±
            if len(group) > 1:
                lines.append("\n  é¡ä¼¼åº¦:")
                for k in range(len(group)):
                    for l in range(k + 1, len(group)):
                        similarity = group[k].similarity_score(group[l])
                        lines.append(f"    {k+1}â†”{l+1}: {similarity:.1%}")
        
        lines.append("")
    
    return '\n'.join(lines)

def generate_csv_report(duplicates: Dict, project_path: str) -> str:
    """CSVå½¢å¼ã®ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
    rows = []
    
    for method_name, groups in duplicates.items():
        for group_idx, group in enumerate(groups, 1):
            for method_idx, method in enumerate(group, 1):
                relative_path = os.path.relpath(method.file_path, project_path)
                
                # æœ€é«˜é¡ä¼¼åº¦ã‚’è¨ˆç®—
                max_similarity = 0
                if len(group) > 1:
                    similarities = [
                        method.similarity_score(other) 
                        for other in group if other != method
                    ]
                    max_similarity = max(similarities) if similarities else 0
                
                rows.append({
                    'ãƒ¡ã‚½ãƒƒãƒ‰å': method_name,
                    'ã‚°ãƒ«ãƒ¼ãƒ—': group_idx,
                    'ãƒ¡ã‚½ãƒƒãƒ‰ç•ªå·': method_idx,
                    'ã‚·ã‚°ãƒãƒãƒ£': method.signature,
                    'ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹': relative_path,
                    'è¡Œç•ªå·': method.line_number,
                    'æœ€é«˜é¡ä¼¼åº¦': f"{max_similarity:.1%}",
                    'ã‚°ãƒ«ãƒ¼ãƒ—ã‚µã‚¤ã‚º': len(group),
                    'Docstring': method.docstring.replace('\n', ' ').strip()[:100]
                })
    
    df = pd.DataFrame(rows)
    return df.to_csv(index=False)

if __name__ == "__main__":
    main()